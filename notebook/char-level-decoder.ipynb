{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import all required libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.contrib.framework.python.framework import checkpoint_utils\n",
    "\n",
    "import random\n",
    "import collections\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define parameters of the program\n",
    "corpus_path = '../data/got_all_edited.txt'\n",
    "\n",
    "num_epoch = 30\n",
    "\n",
    "batch_size = 30\n",
    "num_steps = 60\n",
    "embedding_size = 100\n",
    "\n",
    "hidden_unit_size = 256\n",
    "vocabulary_size = 20000\n",
    "learning_rate = 1e-4\n",
    "\n",
    "sample_length = 10\n",
    "\n",
    "STOP_TOKEN = '*STOP*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a function to load and preprocess the text corpus then return list of chars\n",
    "def read_file(path):\n",
    "    with open(corpus_path) as f:\n",
    "        char_tokens = ['*STOP*']\n",
    "        text = f.read()\n",
    "        char_tokens.extend(text)\n",
    "        \n",
    "        for i in range(len(char_tokens)):\n",
    "            if char_tokens[i] == '\\n':\n",
    "                char_tokens[i] = STOP_TOKEN\n",
    "        \n",
    "        return char_tokens\n",
    "    \n",
    "def build_dataset(tokens):\n",
    "    counts = []\n",
    "    counts.extend(collections.Counter(tokens).most_common())\n",
    "    \n",
    "    dictionary = dict()\n",
    "    data = list()\n",
    "    \n",
    "    for token, _ in counts:\n",
    "        dictionary[token] = len(dictionary)\n",
    "        \n",
    "    for token in tokens:\n",
    "        data.append(dictionary[token])\n",
    "        \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    return data, dictionary, reverse_dictionary\n",
    "\n",
    "def generate_batch(dataset, batch_size, num_steps, offset=0):\n",
    "    assert offset + batch_size * num_steps < len(dataset)\n",
    "    \n",
    "    batch_context = np.ndarray((batch_size, num_steps), dtype=np.int32)\n",
    "    batch_target = np.ndarray((batch_size, num_steps), dtype=np.int32)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        batch_context[i] = dataset[offset : offset+num_steps]\n",
    "        batch_target[i] = dataset[offset+1 : offset+num_steps+1]\n",
    "        offset += num_steps\n",
    "        \n",
    "    return batch_context, batch_target, offset\n",
    "\n",
    "tokens = read_file(corpus_path)\n",
    "data, tokendict, tokendictreversed = build_dataset(tokens)\n",
    "\n",
    "vocabsize = len(tokendict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # setup input and labels placeholders\n",
    "    seed_inputs = tf.placeholder(tf.int32, shape=[1, None])\n",
    "    single_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    prev_state_c = tf.placeholder(tf.float32, shape=[1, 256])\n",
    "    prev_state_h = tf.placeholder(tf.float32, shape=[1, 256])\n",
    "    prev_state = (prev_state_c, prev_state_h)\n",
    "    \n",
    "    bsize = tf.placeholder(tf.int32)\n",
    "    temperature = tf.placeholder(tf.float32)\n",
    "    \n",
    "    logits_weights = tf.Variable(tf.truncated_normal([hidden_unit_size, vocabsize], stddev=0.1), \n",
    "                                     name='Variable_1')\n",
    "    logits_biases = tf.Variable(tf.zeros([vocabsize]),\n",
    "                                   name='Variable_2')\n",
    "    \n",
    "    # instantiate embedding matrix\n",
    "    charvectors = tf.Variable(tf.random_normal([vocabsize, embedding_size]), name='Variable')\n",
    "    seedcharvectors = tf.nn.embedding_lookup(charvectors, seed_inputs)\n",
    "    \n",
    "    rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_unit_size, forget_bias=0.0, state_is_tuple=True)\n",
    "    init_state = rnn_cell.zero_state(bsize, tf.float32)\n",
    "    outs, seed_state = tf.nn.dynamic_rnn(rnn_cell, seedcharvectors, initial_state=init_state)\n",
    "    seed_output = seed_state.h\n",
    "    seed_logits = tf.matmul(seed_output, logits_weights) + logits_biases\n",
    "   \n",
    "    with tf.variable_scope(\"RNN\") as scope:\n",
    "        scope.reuse_variables()\n",
    "        current_input = tf.nn.embedding_lookup(charvectors, single_input)\n",
    "        current_output, current_state = rnn_cell(current_input, prev_state)\n",
    " \n",
    "        logits = tf.matmul(current_output, logits_weights) + logits_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_softmax(logits, temperature=1.0):\n",
    "    logits = logits / temperature\n",
    "    softmax = np.exp(logits) / np.sum(np.exp(logits))\n",
    "    r = random.random() # range: [0,1)\n",
    "    total = 0.0\n",
    "    for i in range(len(softmax)):\n",
    "        total += softmax[i]\n",
    "        if total > r:\n",
    "            return i\n",
    "    return len(softmax)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    var_saver = tf.train.Saver(tf.trainable_variables())\n",
    "    path = 'checkpoints/char_rnn_langmodel.ckpt'\n",
    "    var_saver.restore(sess, path)\n",
    "    \n",
    "    initial = sess.run(init_state, feed_dict={bsize: 1})\n",
    "    \n",
    "    seed_string = [map(lambda x: tokendict[x], 'Winterfell ')]\n",
    "    \n",
    "    feed_dict = {seed_inputs: seed_string,\n",
    "                 bsize: 1, init_state: initial}\n",
    "    [seed_s, seed_l] = sess.run([seed_state, seed_logits], feed_dict=feed_dict)\n",
    "    \n",
    "    # iterate through the length of the sample:\n",
    "    samples = [] + seed_string[0]\n",
    "    current_s = seed_s\n",
    "    current_logits = seed_l\n",
    "    current_inp = sample_softmax(current_logits[0], temperature=0.6)\n",
    "    \n",
    "    samples.append(current_inp)\n",
    "    \n",
    "    for i in range(5000):\n",
    "        feed_dict = {single_input: [current_inp], prev_state_c: current_s.c, prev_state_h: current_s.h}\n",
    "        [current_logits, current_s] = sess.run([logits, current_state], feed_dict=feed_dict)\n",
    "\n",
    "        current_inp = sample_softmax(current_logits[0], temperature=0.6)\n",
    "        samples.append(current_inp)\n",
    "        \n",
    "    print ''.join(map(lambda x: tokendictreversed[x], samples))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}