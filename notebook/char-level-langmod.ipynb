{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character-level RNN Language Model written in Tensorflow.\n",
    "\n",
    "The purpose of this notebook is to demonstrate how we could build Recurrent Neural Network for character language modeling and train them on the George R.R. Martin's Game of Thrones novel series. We will show how this learned language model to able to generate sequences of character that constitute story text with similar style as the GoT's novel text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import all required libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import collections\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function to load and preprocess the text corpus then return list of chars\n",
    "def read_file(path):\n",
    "    with open(corpus_path) as f:\n",
    "        char_tokens = ['*STOP*']\n",
    "        text = f.read()\n",
    "        char_tokens.extend(text)\n",
    "        \n",
    "        for i in range(len(char_tokens)):\n",
    "            if char_tokens[i] == '\\n':\n",
    "                char_tokens[i] = '*STOP*'\n",
    "        \n",
    "        return char_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_dataset(tokens):\n",
    "    counts = []\n",
    "    counts.extend(collections.Counter(tokens).most_common())\n",
    "    \n",
    "    dictionary = dict()\n",
    "    data = list()\n",
    "    \n",
    "    for token, _ in counts:\n",
    "        dictionary[token] = len(dictionary)\n",
    "        \n",
    "    for token in tokens:\n",
    "        data.append(dictionary[token])\n",
    "        \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    return data, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(dataset, batch_size, num_steps, offset=0):\n",
    "    assert offset + batch_size * num_steps < len(dataset)\n",
    "    \n",
    "    batch_context = np.ndarray((batch_size, num_steps), dtype=np.int32)\n",
    "    batch_target = np.ndarray((batch_size, num_steps), dtype=np.int32)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        batch_context[i] = dataset[offset : offset+num_steps]\n",
    "        batch_target[i] = dataset[offset+1 : offset+num_steps+1]\n",
    "        offset += num_steps\n",
    "        \n",
    "    return batch_context, batch_target, offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define parameters of the program\n",
    "corpus_path = '../data/got_all_edited.txt'\n",
    "\n",
    "num_epoch = 30\n",
    "\n",
    "batch_size = 5\n",
    "num_steps = 60\n",
    "embedding_size = 100\n",
    "\n",
    "hidden_unit_size = 256\n",
    "vocabulary_size = 20000\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = read_file(corpus_path)\n",
    "data, tokendict, tokendictreversed = build_dataset(tokens)\n",
    "\n",
    "vocabsize = len(tokendict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, label, _ = generate_batch(data, batch_size, num_steps)\n",
    "for batch_train, batch_label in zip(train, label):\n",
    "    print ''.join([tokendictreversed[token] for token in batch_train]) + ' --> '\n",
    "    print ''.join([tokendictreversed[word] for word in batch_label])\n",
    "    print '----------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # setup input and labels placeholders\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[None, num_steps])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[None, num_steps])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    bsize = tf.placeholder(tf.int32)\n",
    "    \n",
    "    # instantiate embedding matrix\n",
    "    charvectors = tf.Variable(tf.random_normal([vocabsize, embedding_size]))\n",
    "    charvector = tf.nn.embedding_lookup(charvectors, train_inputs)\n",
    "    charvector = tf.nn.dropout(charvector, keep_prob)\n",
    "    \n",
    "    # define the rnn cell and the initial state\n",
    "    rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_unit_size, forget_bias=0.0, state_is_tuple=True)\n",
    "    init_state = rnn_cell.zero_state(bsize, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(rnn_cell, charvector, initial_state=init_state)   \n",
    "        \n",
    "    # reshape the outputs into 2D vectors\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [bsize * num_steps, hidden_unit_size])\n",
    "     \n",
    "    logits_weights = tf.Variable(tf.truncated_normal([hidden_unit_size, vocabsize], stddev=0.1))\n",
    "    logits_biases = tf.Variable(tf.zeros([vocabsize]))\n",
    "    logits = tf.matmul(rnn_outputs, logits_weights) + logits_biases\n",
    "    \n",
    "    loss_weights = tf.ones([batch_size * num_steps])\n",
    "    losses = tf.nn.seq2seq.sequence_loss_by_example([logits], [train_labels], [loss_weights])\n",
    "    loss = tf.reduce_sum(losses) / batch_size\n",
    "        \n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    var_saver = tf.train.Saver(tf.trainable_variables())\n",
    "    path = 'char_rnn_langmodel.ckpt'\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        offset = 0\n",
    "        total_loss = 0.\n",
    "        iters = 0\n",
    "        seqs = 0\n",
    "        training_state = sess.run(init_state, feed_dict={bsize: batch_size})\n",
    "        while offset + batch_size * num_steps < len(data):\n",
    "            batch_context, batch_target, offset = generate_batch(data, batch_size, num_steps, offset)\n",
    "            feed_dict = {train_inputs: batch_context, train_labels: batch_target, \n",
    "                         keep_prob: .5, init_state:training_state,\n",
    "                         bsize: batch_size}\n",
    "            training_loss, training_state, _ = sess.run([loss, final_state, train_step], feed_dict=feed_dict)\n",
    "            \n",
    "            total_loss += training_loss\n",
    "            iters += num_steps\n",
    "        \n",
    "            seqs += batch_size\n",
    "        \n",
    "            if seqs % 1000 == 0:\n",
    "                perplexity = np.exp(total_loss / iters)\n",
    "                print 'Epoch: %d, Tokens trained: %04d, Perplexity: %.4f' % \\\n",
    "                    (epoch+1, offset, perplexity)\n",
    "                # save checkpoint every 40000 train steps\n",
    "                checkpoint_path = var_saver.save(sess, path)\n",
    "                print 'Checkpoint saved as: %s' % (checkpoint_path)\n",
    "                print 'Time to checkpoint: ' + str(time.time() - start_time)\n",
    "                start_time = time.time()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
