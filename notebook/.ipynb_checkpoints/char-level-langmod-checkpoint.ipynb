{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character-level RNN Language Model written in Tensorflow.\n",
    "\n",
    "The purpose of this notebook is to demonstrate how we could build Recurrent Neural Network for character language modeling and train them on the George R.R. Martin's Game of Thrones novel series. We will show how this learned language model to able to generate sequences of character that constitute story text with similar style as the GoT's novel text.\n",
    "\n",
    "To read:\n",
    "- Tensorflow implementation of dynamic rnn, sequence loss by example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import all required libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import collections\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function to load and preprocess the text corpus then return list of chars\n",
    "def read_file(path):\n",
    "    with open(corpus_path) as f:\n",
    "        char_tokens = ['*STOP*']\n",
    "        text = f.read()\n",
    "        char_tokens.extend(text)\n",
    "        \n",
    "        for i in range(len(char_tokens)):\n",
    "            if char_tokens[i] == '\\n':\n",
    "                char_tokens[i] = '*STOP*'\n",
    "        \n",
    "        return char_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_dataset(tokens):\n",
    "    counts = []\n",
    "    counts.extend(collections.Counter(tokens).most_common())\n",
    "    \n",
    "    dictionary = dict()\n",
    "    data = list()\n",
    "    \n",
    "    for token, _ in counts:\n",
    "        dictionary[token] = len(dictionary)\n",
    "        \n",
    "    for token in tokens:\n",
    "        data.append(dictionary[token])\n",
    "        \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    return data, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(dataset, batch_size, num_steps, offset=0):\n",
    "    assert offset + batch_size * num_steps < len(dataset)\n",
    "    \n",
    "    batch_context = np.ndarray((batch_size, num_steps), dtype=np.int32)\n",
    "    batch_target = np.ndarray((batch_size, num_steps), dtype=np.int32)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        batch_context[i] = dataset[offset : offset+num_steps]\n",
    "        batch_target[i] = dataset[offset+1 : offset+num_steps+1]\n",
    "        offset += num_steps\n",
    "        \n",
    "    return batch_context, batch_target, offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define parameters of the program\n",
    "corpus_path = '../data/got_all_edited.txt'\n",
    "\n",
    "num_epoch = 30\n",
    "\n",
    "batch_size = 30\n",
    "num_steps = 60\n",
    "embedding_size = 100\n",
    "\n",
    "hidden_unit_size = 256\n",
    "vocabulary_size = 20000\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = read_file(corpus_path)\n",
    "data, tokendict, tokendictreversed = build_dataset(tokens)\n",
    "\n",
    "vocabsize = len(tokendict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*STOP*\"We should start back,\" Gared urged as the woods began to g --> \n",
      "\"We should start back,\" Gared urged as the woods began to gr\n",
      "----------\n",
      "row dark around them. \"The wildlings are dead.\"*STOP*\"Do the dead --> \n",
      "ow dark around them. \"The wildlings are dead.\"*STOP*\"Do the dead \n",
      "----------\n",
      " frighten you?\" Ser Waymar Royce asked with just the hint of --> \n",
      "frighten you?\" Ser Waymar Royce asked with just the hint of \n",
      "----------\n",
      " a smile.*STOP*Gared did not rise to the bait. He was an old man, --> \n",
      "a smile.*STOP*Gared did not rise to the bait. He was an old man, \n",
      "----------\n",
      " past fifty, and he had seen the lordlings come and go. \"Dea --> \n",
      "past fifty, and he had seen the lordlings come and go. \"Dead\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "train, label, _ = generate_batch(data, 5, num_steps)\n",
    "for batch_train, batch_label in zip(train, label):\n",
    "    print ''.join([tokendictreversed[token] for token in batch_train]) + ' --> '\n",
    "    print ''.join([tokendictreversed[word] for word in batch_label])\n",
    "    print '----------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # setup input and labels placeholders\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[None, num_steps])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[None, num_steps])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    bsize = tf.placeholder(tf.int32)\n",
    "    \n",
    "    # instantiate embedding matrix\n",
    "    charvectors = tf.Variable(tf.random_normal([vocabsize, embedding_size]))\n",
    "    charvector = tf.nn.embedding_lookup(charvectors, train_inputs)\n",
    "    charvector = tf.nn.dropout(charvector, keep_prob)\n",
    "    \n",
    "    # define the rnn cell and the initial state\n",
    "    rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_unit_size, forget_bias=0.0, state_is_tuple=True)\n",
    "    init_state = rnn_cell.zero_state(bsize, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(rnn_cell, charvector, initial_state=init_state)   \n",
    "        \n",
    "    # reshape the outputs into 2D vectors\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [bsize * num_steps, hidden_unit_size])\n",
    "     \n",
    "    logits_weights = tf.Variable(tf.truncated_normal([hidden_unit_size, vocabsize], stddev=0.1))\n",
    "    logits_biases = tf.Variable(tf.zeros([vocabsize]))\n",
    "    logits = tf.matmul(rnn_outputs, logits_weights) + logits_biases\n",
    "    \n",
    "    loss_weights = tf.ones([batch_size * num_steps])\n",
    "    losses = tf.nn.seq2seq.sequence_loss_by_example([logits], [train_labels], [loss_weights])\n",
    "    loss = tf.reduce_sum(losses) / batch_size\n",
    "        \n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Time elapsed: 5.29 s, Tokens trained: 180000, Perplexity: 45.0148\n",
      "Epoch: 1, Time elapsed: 10.33 s, Tokens trained: 360000, Perplexity: 27.4767\n",
      "Epoch: 1, Time elapsed: 15.39 s, Tokens trained: 540000, Perplexity: 21.8503\n",
      "Epoch: 1, Time elapsed: 20.38 s, Tokens trained: 720000, Perplexity: 18.9705\n",
      "Epoch: 1, Time elapsed: 25.38 s, Tokens trained: 900000, Perplexity: 17.2149\n",
      "Epoch: 1, Time elapsed: 30.42 s, Tokens trained: 1080000, Perplexity: 15.9540\n",
      "Epoch: 1, Time elapsed: 35.43 s, Tokens trained: 1260000, Perplexity: 15.0330\n",
      "Epoch: 1, Time elapsed: 40.49 s, Tokens trained: 1440000, Perplexity: 14.3094\n",
      "Epoch: 1, Time elapsed: 45.72 s, Tokens trained: 1620000, Perplexity: 13.7029\n",
      "Epoch: 1, Time elapsed: 50.86 s, Tokens trained: 1800000, Perplexity: 13.2236\n",
      "Epoch: 1, Time elapsed: 55.92 s, Tokens trained: 1980000, Perplexity: 12.8161\n",
      "Epoch: 1, Time elapsed: 61.07 s, Tokens trained: 2160000, Perplexity: 12.4699\n",
      "Epoch: 1, Time elapsed: 66.13 s, Tokens trained: 2340000, Perplexity: 12.1619\n",
      "Epoch: 1, Time elapsed: 71.29 s, Tokens trained: 2520000, Perplexity: 11.8883\n",
      "Epoch: 1, Time elapsed: 76.55 s, Tokens trained: 2700000, Perplexity: 11.6536\n",
      "Epoch: 1, Time elapsed: 81.74 s, Tokens trained: 2880000, Perplexity: 11.4205\n",
      "Epoch: 1, Time elapsed: 86.91 s, Tokens trained: 3060000, Perplexity: 11.2060\n",
      "Epoch: 1, Time elapsed: 92.10 s, Tokens trained: 3240000, Perplexity: 11.0260\n",
      "Epoch: 1, Time elapsed: 97.69 s, Tokens trained: 3420000, Perplexity: 10.9873\n",
      "Epoch: 1, Time elapsed: 102.76 s, Tokens trained: 3600000, Perplexity: 10.8442\n",
      "Epoch: 1, Time elapsed: 108.10 s, Tokens trained: 3780000, Perplexity: 10.6945\n",
      "Epoch: 1, Time elapsed: 113.15 s, Tokens trained: 3960000, Perplexity: 10.5553\n",
      "Epoch: 1, Time elapsed: 118.32 s, Tokens trained: 4140000, Perplexity: 10.4241\n",
      "Epoch: 1, Time elapsed: 123.67 s, Tokens trained: 4320000, Perplexity: 10.2991\n",
      "Epoch: 1, Time elapsed: 128.72 s, Tokens trained: 4500000, Perplexity: 10.1879\n",
      "Epoch: 1, Time elapsed: 133.77 s, Tokens trained: 4680000, Perplexity: 10.0792\n",
      "Epoch: 1, Time elapsed: 138.79 s, Tokens trained: 4860000, Perplexity: 9.9846\n",
      "Epoch: 1, Time elapsed: 143.81 s, Tokens trained: 5040000, Perplexity: 9.8885\n",
      "Epoch: 1, Time elapsed: 148.82 s, Tokens trained: 5220000, Perplexity: 9.7966\n",
      "Epoch: 1, Time elapsed: 154.14 s, Tokens trained: 5400000, Perplexity: 9.7129\n",
      "Epoch: 1, Time elapsed: 159.50 s, Tokens trained: 5580000, Perplexity: 9.6250\n",
      "Epoch: 1, Time elapsed: 164.76 s, Tokens trained: 5760000, Perplexity: 9.6364\n",
      "Epoch: 1, Time elapsed: 169.87 s, Tokens trained: 5940000, Perplexity: 9.5603\n",
      "Epoch: 1, Time elapsed: 174.89 s, Tokens trained: 6120000, Perplexity: 9.4877\n",
      "Epoch: 1, Time elapsed: 179.91 s, Tokens trained: 6300000, Perplexity: 9.4237\n",
      "Epoch: 1, Time elapsed: 184.96 s, Tokens trained: 6480000, Perplexity: 9.3605\n",
      "Epoch: 1, Time elapsed: 189.97 s, Tokens trained: 6660000, Perplexity: 9.2972\n",
      "Epoch: 1, Time elapsed: 194.98 s, Tokens trained: 6840000, Perplexity: 9.2371\n",
      "Epoch: 1, Time elapsed: 199.98 s, Tokens trained: 7020000, Perplexity: 9.1769\n",
      "Epoch: 1, Time elapsed: 205.43 s, Tokens trained: 7200000, Perplexity: 9.1184\n",
      "Epoch: 1, Time elapsed: 210.53 s, Tokens trained: 7380000, Perplexity: 9.1388\n",
      "Epoch: 1, Time elapsed: 215.56 s, Tokens trained: 7560000, Perplexity: 9.0870\n",
      "Epoch: 1, Time elapsed: 220.59 s, Tokens trained: 7740000, Perplexity: 9.0383\n",
      "Epoch: 1, Time elapsed: 225.60 s, Tokens trained: 7920000, Perplexity: 8.9902\n",
      "Epoch: 1, Time elapsed: 230.61 s, Tokens trained: 8100000, Perplexity: 8.9436\n",
      "Epoch: 1, Time elapsed: 235.63 s, Tokens trained: 8280000, Perplexity: 8.8984\n",
      "Epoch: 1, Time elapsed: 240.80 s, Tokens trained: 8460000, Perplexity: 8.8498\n",
      "Epoch: 1, Time elapsed: 246.38 s, Tokens trained: 8640000, Perplexity: 8.8050\n",
      "Epoch: 1, Time elapsed: 251.58 s, Tokens trained: 8820000, Perplexity: 8.7618\n",
      "Epoch: 1, Time elapsed: 256.78 s, Tokens trained: 9000000, Perplexity: 8.7225\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    var_saver = tf.train.Saver(tf.trainable_variables())\n",
    "    path = 'checkpoints/char_rnn_langmodel.ckpt'\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        offset = 0\n",
    "        total_loss = 0.\n",
    "        iters = 0\n",
    "        seqs = 0\n",
    "        training_state = sess.run(init_state, feed_dict={bsize: batch_size})\n",
    "        while offset + batch_size * num_steps < len(data):\n",
    "            batch_context, batch_target, offset = generate_batch(data, batch_size, num_steps, offset)\n",
    "            feed_dict = {train_inputs: batch_context, train_labels: batch_target, \n",
    "                         keep_prob: .5, init_state:training_state,\n",
    "                         bsize: batch_size}\n",
    "            training_loss, training_state, _ = sess.run([loss, final_state, train_step], feed_dict=feed_dict)\n",
    "            \n",
    "            total_loss += training_loss\n",
    "            iters += num_steps\n",
    "        \n",
    "            seqs += batch_size\n",
    "        \n",
    "            if seqs % 1000 == 0:\n",
    "                perplexity = np.exp(total_loss / iters)\n",
    "                print 'Epoch: %d, Time elapsed: %.2f s, Tokens trained: %04d, Perplexity: %.4f' % \\\n",
    "                    (epoch+1, (time.time() - start_time), offset, perplexity)\n",
    "                    \n",
    "        # save checkpoint every 10000 train steps\n",
    "        checkpoint_path = var_saver.save(sess, path)\n",
    "        print 'Epoch completed. Checkpoint saved as: %s' % (checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
