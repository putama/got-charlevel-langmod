{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character-level RNN Language Model written in Tensorflow.\n",
    "\n",
    "The purpose of this notebook is to demonstrate how we could build Recurrent Neural Network for character language modeling and train them on the George R.R. Martin's Game of Thrones novel series. We will show how this learned language model to able to generate sequences of character that constitute story text with similar style as the GoT's novel text.\n",
    "\n",
    "To read:\n",
    "- Tensorflow implementation of dynamic rnn, sequence loss by example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import all required libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import collections\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define parameters of the program\n",
    "corpus_path = '../data/got_all_edited.txt'\n",
    "\n",
    "num_epoch = 30\n",
    "\n",
    "batch_size = 30\n",
    "num_steps = 60\n",
    "embedding_size = 100\n",
    "\n",
    "hidden_unit_size = 256\n",
    "vocabulary_size = 20000\n",
    "learning_rate = 1e-4\n",
    "\n",
    "STOP_TOKEN = '*STOP*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function to load and preprocess the text corpus then return list of chars\n",
    "def read_file(path):\n",
    "    with open(corpus_path) as f:\n",
    "        char_tokens = ['*STOP*']\n",
    "        text = f.read()\n",
    "        char_tokens.extend(text)\n",
    "        \n",
    "        for i in range(len(char_tokens)):\n",
    "            if char_tokens[i] == '\\n':\n",
    "                char_tokens[i] = STOP_TOKEN\n",
    "        \n",
    "        return char_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_dataset(tokens):\n",
    "    counts = []\n",
    "    counts.extend(collections.Counter(tokens).most_common())\n",
    "    \n",
    "    dictionary = dict()\n",
    "    data = list()\n",
    "    \n",
    "    for token, _ in counts:\n",
    "        dictionary[token] = len(dictionary)\n",
    "        \n",
    "    for token in tokens:\n",
    "        data.append(dictionary[token])\n",
    "        \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    return data, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(dataset, batch_size, num_steps, offset=0):\n",
    "    assert offset + batch_size * num_steps < len(dataset)\n",
    "    \n",
    "    batch_context = np.ndarray((batch_size, num_steps), dtype=np.int32)\n",
    "    batch_target = np.ndarray((batch_size, num_steps), dtype=np.int32)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        batch_context[i] = dataset[offset : offset+num_steps]\n",
    "        batch_target[i] = dataset[offset+1 : offset+num_steps+1]\n",
    "        offset += num_steps\n",
    "        \n",
    "    return batch_context, batch_target, offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = read_file(corpus_path)\n",
    "data, tokendict, tokendictreversed = build_dataset(tokens)\n",
    "\n",
    "vocabsize = len(tokendict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split the data to training set and held out set\n",
    "for i in range(int(0.8*len(data)), len(data)):\n",
    "    if data[i] == tokendict[STOP_TOKEN]:\n",
    "        traindata = data[0:i]\n",
    "        devdata = data[i:len(data)]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, label, _ = generate_batch(data, 5, num_steps)\n",
    "for batch_train, batch_label in zip(train, label):\n",
    "    print ''.join([tokendictreversed[token] for token in batch_train]) + ' --> '\n",
    "    print ''.join([tokendictreversed[word] for word in batch_label])\n",
    "    print '----------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # setup input and labels placeholders\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[None, num_steps])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[None, num_steps])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    bsize = tf.placeholder(tf.int32)\n",
    "    \n",
    "    # instantiate embedding matrix\n",
    "    charvectors = tf.Variable(tf.random_normal([vocabsize, embedding_size]))\n",
    "    charvector = tf.nn.embedding_lookup(charvectors, train_inputs)\n",
    "    charvector = tf.nn.dropout(charvector, keep_prob)\n",
    "    \n",
    "    # define the rnn cell and the initial state\n",
    "    rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_unit_size, forget_bias=0.0, state_is_tuple=True)\n",
    "    init_state = rnn_cell.zero_state(bsize, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(rnn_cell, charvector, initial_state=init_state)   \n",
    "        \n",
    "    # reshape the outputs into 2D vectors\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [bsize * num_steps, hidden_unit_size])\n",
    "     \n",
    "    logits_weights = tf.Variable(tf.truncated_normal([hidden_unit_size, vocabsize], stddev=0.1))\n",
    "    logits_biases = tf.Variable(tf.zeros([vocabsize]))\n",
    "    logits = tf.matmul(rnn_outputs, logits_weights) + logits_biases\n",
    "    \n",
    "    loss_weights = tf.ones([batch_size * num_steps])\n",
    "    losses = tf.nn.seq2seq.sequence_loss_by_example([logits], [train_labels], [loss_weights])\n",
    "    loss = tf.reduce_sum(losses) / batch_size\n",
    "        \n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    var_saver = tf.train.Saver(tf.trainable_variables())\n",
    "    path = 'checkpoints/char_rnn_langmodel.ckpt'\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        ############ train the model on the training set ######################\n",
    "        offset = 0\n",
    "        total_loss = 0.\n",
    "        iters = 0\n",
    "        seqs = 0\n",
    "        training_state = sess.run(init_state, feed_dict={bsize: batch_size})\n",
    "        while offset + batch_size * num_steps < len(data):\n",
    "            batch_context, batch_target, offset = generate_batch(data, batch_size, num_steps, offset)\n",
    "            feed_dict = {train_inputs: batch_context, train_labels: batch_target, \n",
    "                         keep_prob: .5, init_state:training_state,\n",
    "                         bsize: batch_size}\n",
    "            training_loss, training_state, _ = sess.run([loss, final_state, train_step], feed_dict=feed_dict)\n",
    "            \n",
    "            total_loss += training_loss\n",
    "            iters += num_steps\n",
    "        \n",
    "            seqs += batch_size\n",
    "        \n",
    "            if seqs % 5000 == 0:\n",
    "                perplexity = np.exp(total_loss / iters)\n",
    "                print 'Epoch: %d, Time elapsed: %.2f s, Tokens trained: %04d, Perplexity: %.4f' % \\\n",
    "                    (epoch+1, (time.time() - start_time), offset, perplexity)\n",
    "                    \n",
    "        ############ evaluate the trained model on development set #############\n",
    "        dev_offset = 0\n",
    "        dev_total_loss = 0.\n",
    "        dev_iters = 0\n",
    "        dev_seqs = 0\n",
    "        dev_state = sess.run(init_state, feed_dict={bsize: batch_size})\n",
    "        while dev_offset + batch_size * num_steps < len(data):\n",
    "            batch_context, batch_target, dev_offset = generate_batch(data, batch_size, num_steps, dev_offset)\n",
    "            feed_dict = {train_inputs: batch_context, train_labels: batch_target, \n",
    "                         keep_prob: .5, init_state:training_state,\n",
    "                         bsize: batch_size}\n",
    "            training_loss, training_state, _ = sess.run([loss, final_state, train_step], feed_dict=feed_dict)\n",
    "            \n",
    "            dev_total_loss += training_loss\n",
    "            dev_iters += num_steps\n",
    "        \n",
    "            dev_seqs += batch_size\n",
    "        \n",
    "            if dev_seqs % 5000 == 0:\n",
    "                perplexity = np.exp(dev_total_loss / dev_iters)\n",
    "                print 'Epoch: %d, Time elapsed: %.2f s, Tokens trained: %04d, Perplexity: %.4f' % \\\n",
    "                    (epoch+1, (time.time() - start_time), dev_offset, perplexity)\n",
    "                    \n",
    "        # save checkpoint every 10000 train steps\n",
    "        checkpoint_path = var_saver.save(sess, path)\n",
    "        print 'Epoch completed. Checkpoint saved as: %s' % (checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
